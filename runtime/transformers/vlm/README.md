# Vision Language Model (VLM) Runtime Inference

This directory contains the runtime inference code for running the compiled Qwen2-VL-2B-Instruct model on Aries 2 hardware using the Mobilint runtime.

## Overview

After compiling the VLM model using the compilation tutorial (in `/workspace/mblt-sdk-tutorial/compilation/transformers/vlm/`), you can run inference on the compiled MXQ models using this runtime script.

The inference script demonstrates how to:
- Load compiled MXQ models from the compilation output directory
- Use the Mobilint runtime pipeline for image-text-to-text tasks
- Run inference with streaming output
- Handle vision-language queries with images

## Prerequisites

Before running inference, ensure you have:
- **Completed the compilation tutorial** - All 4 files must be generated in the compilation directory
- **mblt-model-zoo package installed**:
  ```bash
  pip install mblt-model-zoo
  ```
- **Required dependencies**:
  ```bash
  pip install transformers==4.54.0 torch pillow
  ```

## Required Files

The inference script expects the following 4 files to be present in the compilation output directory (`/workspace/mblt-sdk-tutorial/compilation/transformers/vlm/compile/mxq/`):

1. **Qwen2-VL-2B-Instruct_text_model.mxq** - Compiled language model
2. **Qwen2-VL-2B-Instruct_vision_transformer.mxq** - Compiled vision encoder
3. **config.json** - Model configuration with MXQ paths
4. **model.safetensors** - Rotated embedding weights

These files are automatically generated by following the compilation tutorial.

## Running Inference

### Basic Usage

Simply run the inference script:

```bash
cd /workspace/mblt-sdk-tutorial/runtime/transformers/vlm
python run_qwen2_vl_local.py
```

The script will:
1. Load the compiled MXQ models from the compilation output directory
2. Load the processor from HuggingFace (model ID: `mobilint/Qwen2-VL-2B-Instruct`)
3. Run inference on a demo image with a sample prompt
4. Stream the generated text output in real-time

### Understanding the Code

The `run_qwen2_vl_local.py` script demonstrates the complete inference workflow:

```python
from transformers import TextStreamer
from mblt_model_zoo.transformers import pipeline, AutoModelForImageTextToText, AutoProcessor

# Path to compiled MXQ models
model_folder = "/workspace/mblt-sdk-tutorial/compilation/transformers/vlm/compile/mxq/"
model_id = "mobilint/Qwen2-VL-2B-Instruct"

# Load compiled model
model = AutoModelForImageTextToText.from_pretrained(model_folder)

# Load processor from HuggingFace
processor = AutoProcessor.from_pretrained(model_id)

# Create pipeline
pipe = pipeline(
    "image-text-to-text",
    model=model,
    processor=processor,
)

# Prepare messages with image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://..."},
            {"type": "text", "text": "Your question here"},
        ],
    }
]

# Run inference with streaming
pipe(
    text=messages,
    generate_kwargs={
        "max_length": 512,
        "streamer": TextStreamer(tokenizer=pipe.tokenizer, skip_prompt=False),
    },
)

# Clean up
pipe.model.dispose()
```

### Key Components

#### 1. Model Loading
- **AutoModelForImageTextToText.from_pretrained()** - Loads the compiled MXQ models from the specified directory
- Automatically detects and loads both language and vision models based on config.json

#### 2. Processor Loading
- **AutoProcessor.from_pretrained()** - Loads the tokenizer and image processor from HuggingFace
- Uses the model ID `mobilint/Qwen2-VL-2B-Instruct` for processor configuration

#### 3. Pipeline Creation
- **pipeline()** - Creates an image-text-to-text pipeline
- Automatically handles the interaction between vision and language components

#### 4. Message Format
- Messages use a structured format with roles and content types
- Content can include both images and text
- Images can be specified as URLs or local file paths

#### 5. Generation
- **generate_kwargs** - Controls generation parameters
- **TextStreamer** - Displays generated text in real-time
- **max_length** - Controls the maximum output length

## Customizing Inference

### Using Local Images

To use a local image instead of a URL:

```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "/path/to/your/image.jpg"},
            {"type": "text", "text": "What is in this image?"},
        ],
    }
]
```

### Changing the Prompt

Modify the text content to ask different questions:

```python
{"type": "text", "text": "Describe the environment and context surrounding the main subject."}
{"type": "text", "text": "What objects are visible in this image?"}
{"type": "text", "text": "Count the number of people in the image."}
{"type": "text", "text": "What is the spatial relationship between objects?"}
```

### Adjusting Generation Parameters

Control the generation behavior with different parameters:

```python
pipe(
    text=messages,
    generate_kwargs={
        "max_length": 1024,      # Longer responses
        "temperature": 0.7,       # Creativity control (higher = more creative)
        "top_p": 0.9,            # Nucleus sampling
        "top_k": 50,             # Top-K sampling
        "repetition_penalty": 1.1, # Penalize repetition
        "streamer": TextStreamer(tokenizer=pipe.tokenizer, skip_prompt=False),
    },
)
```

### Multi-turn Conversations

The model supports multi-turn dialogues:

```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "image.jpg"},
            {"type": "text", "text": "What's in this image?"},
        ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "I see a dog playing in a park."}],
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "What color is the dog?"}],
    },
]
```

### Processing Multiple Images

You can include multiple images in a single query:

```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "image1.jpg"},
            {"type": "image", "image": "image2.jpg"},
            {"type": "text", "text": "What are the differences between these two images?"},
        ],
    }
]
```

## Performance Notes

- **Hardware Acceleration**: The compiled MXQ models run on Aries 2 hardware for optimal performance
- **Stateful KV Cache**: Memory is efficiently managed through the stateful KV cache system
- **Streaming Output**: Text generation is streamed in real-time for better user experience
- **Resource Cleanup**: Always call `pipe.model.dispose()` to properly clean up resources after inference

## Configuration

### Model Path

The script uses an absolute path to the compiled models:

```python
model_folder = "/workspace/mblt-sdk-tutorial/compilation/transformers/vlm/compile/mxq/"
```

If you've compiled the models to a different location, update this path accordingly.

### Model ID

The processor is loaded from HuggingFace using the model ID:

```python
model_id = "mobilint/Qwen2-VL-2B-Instruct"
```

This ID is used to download the tokenizer and image processor configuration.

## Troubleshooting

### Model Not Found Error
```
FileNotFoundError: Model files not found
```
**Solution**: Ensure you've completed the compilation tutorial and all 4 files exist in the compilation output directory.

### Out of Memory (OOM) Errors
**Solutions**:
- Reduce `max_length` in generation parameters
- Process smaller images
- Close other GPU-intensive applications

### Processor Download Issues
**Solutions**:
- Check your internet connection
- Verify HuggingFace access (may require authentication)
- Use `huggingface-cli login` if needed

### Import Errors
```
ModuleNotFoundError: No module named 'mblt_model_zoo'
```
**Solution**: Install the required package:
```bash
pip install mblt-model-zoo
```

## Example Outputs

### Image Description
**Input**: "Describe the environment and context surrounding the main subject."
**Output**: Detailed description of the scene, objects, and spatial relationships

### Object Counting
**Input**: "How many people are in this image?"
**Output**: Count and description of people visible

### Visual Reasoning
**Input**: "What is the person doing?"
**Output**: Analysis of actions and activities in the image

### Spatial Understanding
**Input**: "Where is the dog located relative to the tree?"
**Output**: Description of spatial relationships between objects

## References

- [Compilation Tutorial](../../../compilation/transformers/vlm/README.md)
- [Qwen2-VL Model Card](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
- [Mobilint Documentation](https://docs.mobilint.com)
- [mblt-model-zoo Documentation](https://docs.mobilint.com/model-zoo)

## Support

For issues or questions:
- Check the troubleshooting section above
- Review the compilation tutorial to ensure models are properly compiled
- Refer to the mblt-model-zoo documentation
- Contact Mobilint support with detailed error logs

---

**Note**: This runtime inference script requires properly compiled MXQ models from the compilation tutorial. Make sure to complete the compilation process before attempting to run inference.
